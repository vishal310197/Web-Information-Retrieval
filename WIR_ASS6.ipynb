{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url1 = 'https://www.yorku.ca/mack/chi03d.html'\n",
    "r = requests.get(url1)\n",
    "open('doc1.html' , 'wb').write(r.content)\n",
    "doc1 = open('doc1.html').read()\n",
    "\n",
    "\n",
    "url2 = 'https://www.yorku.ca/mack/uais2006.html'\n",
    "r = requests.get(url2)\n",
    "open('doc2.html' , 'wb').write(r.content)\n",
    "doc2 = open('doc2.html').read()\n",
    "\n",
    "\n",
    "url3 = ' https://www.yorku.ca/mack/etra2014.html'\n",
    "r = requests.get(url3)\n",
    "open('doc3.html' , 'wb').write(r.content)\n",
    "doc3 = open('doc3.html').read()\n",
    "\n",
    "\n",
    "url4 = 'https://www.yorku.ca/mack/hcii2018b.html'\n",
    "r = requests.get(url4)\n",
    "open('doc4.html' , 'wb').write(r.content)\n",
    "doc4 = open('doc4.html').read()\n",
    "\n",
    "\n",
    "url5 = 'https://www.yorku.ca/mack/hcii2007.html'\n",
    "r = requests.get(url5)\n",
    "open('doc5.html' , 'wb').write(r.content)\n",
    "doc5 = open('doc5.html').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>\n",
      "auditory and visual feedback during eye typing\n",
      "\n",
      "<title>\n",
      "effects of feedback and dwell time on eye typing speed and accuracy\n",
      "\n",
      "<title>the use of gaze to control drones\n",
      "\n",
      "<title>trackmaze: a comparison of head-tracking, eye-tracking, and tilt as input \n",
      "methods for mobile games\n",
      "\n",
      "<title>\n",
      "evaluating eye tracking with iso 9241 - part 9\n",
      "\n",
      "abstract</b><br>\n",
      "we describe a study on how auditory and visual feedback affects eye typing. results show that the feedback method influences both text entry speed and error rate. in addition, a proper feedback mode facilitates eye typing by reducing the user's need to switch her gaze between the on-screen keyboard and the typed text field.\n",
      "<p>\n",
      "\n",
      "<b>\n",
      "abstract:</b> eye typing provides a means of communication\n",
      "that is especially useful for people with disabilities.\n",
      "however, most related research addresses technical issues\n",
      "in eye typing systems, and largely ignores design\n",
      "issues. this paper reports experiments studying the impact\n",
      "of auditory and visual feedback on user performance\n",
      "and experience. results show that feedback\n",
      "impacts typing speed, accuracy, gaze behavior, and\n",
      "subjective experience. also, the feedback should be\n",
      "matched with the dwell time. short dwell times require\n",
      "simplified feedback to support the typing rhythm,\n",
      "whereas long dwell times allow extra information on the\n",
      "eye typing process. both short and long dwell times\n",
      "benefit from combined visual and auditory feedback. six\n",
      "guidelines for designing feedback for gaze-based text\n",
      "entry are provided.\n",
      "<p>\n",
      "\n",
      "<b>\n",
      "abstract</b><br>\n",
      "this paper presents an experimental investigation of gaze-based control modes \n",
      "for unmanned aerial vehicles (uavs or “drones”). ten participants performed a \n",
      "simple flying task. we gathered empirical measures, including task completion \n",
      "time, and examined the user experience for difficulty, reliability, and fun. \n",
      "four control modes were tested, with each mode applying a combination of x-y \n",
      "gaze movement and manual (keyboard) input to control speed (pitch), altitude, \n",
      "rotation (yaw), and drafting (roll). participants had similar task completion \n",
      "times for all four control modes, but one combination was considered \n",
      "significantly more reliable than the others. we discuss design and performance \n",
      "issues for the gaze-plus-manual split of controls when drones are operated \n",
      "using gaze in conjunction with tablets, near-eye displays (glasses), or \n",
      "monitors.<p>\n",
      "\n",
      "<b>cr categories:</b> information interfaces and presentation (e.g., hci): \n",
      "miscellaneous.<p>\n",
      "\n",
      "<b>\n",
      "abstract</b>. a user study was performed to compare three input methods (tilt, \n",
      "eye-tracking, head-tracking) with two gain levels (low, high) on a custom-made \n",
      "<i>trackmaze</i> mobile game. the task involved maneuvering a virtual ball through a maze \n",
      "while trying to avoid walls. the game was developed in swift using the arkit \n",
      "framework. the truedepth front-facing camera of an apple <i>iphone x</i> was used for the \n",
      "eye-tracking and head-tracking conditions. we evaluated user performance (maze \n",
      "completion time, number of wall hits) and qualitative measures (ease of use, \n",
      "enjoyment, fatigue). tilt input showed the best performance and eye-tracking \n",
      "showed the worst performance. the mean maze completion time was 12.3 seconds for \n",
      "tilt, 22.5 seconds for head-tracking, and 31.8 seconds for eye-tracking. high gain \n",
      "was 26% faster than low gain. tilt was the most precise input method with only \n",
      "1.06 wall hits per trial, compared to head-tracking (2.30) and eye-tracking \n",
      "(4.00). participants preferred tilt and head-tracking over eye-tracking and noted \n",
      "that the eye-tracking interface was fatiguing and hard to use.\n",
      "<p>\n",
      "\n",
      "<b>\n",
      "abstract</b><br>\n",
      "the iso 9241-9 standard for computer pointing devices proposes an \n",
      "evaluation of performance and comfort [4]. this paper is the first eye tracking \n",
      "evaluation conforming to iso 9241-9. we evaluated three techniques and \n",
      "compared them with a standard mouse. the evaluation used throughput (in \n",
      "bits/s) as a measurement of user performance in a multi-directional point-select \n",
      "task. the \"eye tracking long\" technique required participants to look at an \n",
      "on-screen target and dwell on it for 750 ms for selection. results revealed a lower \n",
      "throughput than for the \"eye tracking short\" technique with a 500 ms dwell \n",
      "time. the \"eye+spacebar\" technique allowed participants to \"point\" with the \n",
      "eye and \"select\" by pressing the spacebar upon fixation. this eliminated the \n",
      "need to wait for selection. it was the best among the three eye tracking \n",
      "techniques with a throughput of 3.78 bits/s, which was close to the 4.68 bits/s \n",
      "for the mouse.\n",
      "<p>\n",
      "\n",
      "<b>\n",
      "introduction\n",
      "</h2>\n",
      "<p>\n",
      "for people with severe disabilities their eyes may be the only means for communication. even though eye typing has been studied for many years, there is little research on design issues [2]. our goal was to study how feedback could facilitate the tedious [1] eye typing task and make gaze-based computer-aided communication more practical for those who need it.\n",
      "<p>\n",
      "\n",
      "<h3>\n",
      "feedback modes\n",
      "</h3>\n",
      "\n",
      "<p>\n",
      "during eye typing the user first focuses on the desired letter. to select the focused letter she continues to fixate on it thus using dwell time as an activation command. feedback is given for focus and selection. the following four feedback modes were tested. \n",
      "<p>\n",
      "\n",
      "<b>visual only.</b> in the visual only mode, the key is highlighted on focus (the 2nd key on left in figure 1) and its symbol shrinks as dwell time elapses. the shrinking draws the attention in, helping the user focus on the center of the key. on selection the letter turns red and the key goes down. \n",
      "<p>\n",
      "\n",
      "<center>\n",
      "<img src=\"chi03d-f1.gif\"><br>\n",
      "<b>figure 1.</b> animation for visual only feedback mode \n",
      "</center>\n",
      "<p>\n",
      "\n",
      "<b>speech only.</b> the speech only mode did not use visual feedback. the symbol on the key was spoken on selection.\n",
      "<p>\n",
      "\n",
      "<b>click plus visual.</b> the click plus visual mode uses two modalities; it has the same visual feedback seen in figure 1 and, in addition, a short \"click\" is heard on selection.\n",
      "<p>\n",
      "\n",
      "<b>speech plus visual.</b> the speech plus visual mode again uses the same visual feedback plus the symbol on the key is spoken on selection.\n",
      "<p>\n",
      "the dwell time was the same for all modes: 400 ms before the start of the focus and 900 ms before selection. a summary of the modes follows.\n",
      "\n",
      "<center>\n",
      "<table bgcolor=\"#f0f0f0\" border=1 cellpadding=5 cellspacing=0>\n",
      "<tr bgcolor=\"#e0e0e0\">\n",
      "<th align=center>feedback mode\n",
      "<th align=center>while focused\n",
      "<th align=center>when selected\n",
      "\n",
      "<tr>\n",
      "<td align=left>visual only\n",
      "<td align=left>shrinking letter\n",
      "<td align=left>red letter, key down\n",
      "\n",
      "<tr>\n",
      "<td align=left>speech only\n",
      "<td align=left>none\n",
      "<td align=left>letter spoken\n",
      "\n",
      "<tr>\n",
      "<td align=left>click + visual\n",
      "<td align=left>shrinking letter\n",
      "<td align=left>red letter, key down, click\n",
      "\n",
      "<tr>\n",
      "<td align=left>speech + visual\n",
      "<td align=left>shrinking letter\n",
      "<td align=left>red letter, key down,<br> \n",
      "letter spoken\n",
      "</table>\n",
      "</center>\n",
      "\n",
      "<h2>\n",
      "\n",
      "introduction</h2>\n",
      "<p>\n",
      "\n",
      "eye typing refers to the production of text using the\n",
      "focus of the eye (aka gaze) as a means of input. it is\n",
      "especially needed by people with severe disabilities,\n",
      "where controlling the eyes is sometimes the only means\n",
      "of interaction with the world. their need for an effective\n",
      "communication system is acute.\n",
      "<p>\n",
      "\n",
      "research on technical aspects of eye typing extends\n",
      "over 20 years. however, there is little research on design\n",
      "issues [16]. the authors’ work [17, 18, 19] is an attempt\n",
      "to partly fill this gap. such work investigated how feedback\n",
      "can facilitate the tedious [5] eye typing task. this\n",
      "paper summarizes the results of three experiments\n",
      "studying various aspects of feedback during eye typing.\n",
      "because gaze has some unique features, the paper\n",
      "briefly introduces gaze as an input method. this is followed\n",
      "by a description of eye typing. examples of the\n",
      "most relevant research on eye gaze feedback are reviewed.\n",
      "the methods and results of the experiments are\n",
      "then presented, followed by guidelines gleaned from the\n",
      "results.\n",
      "<p>\n",
      "\n",
      "<h3>1.1 gaze input</h3>\n",
      "<p>\n",
      "\n",
      "gaze is naturally used to obtain visual information. for\n",
      "example, gaze location shows the focus of attention [13].\n",
      "as an input method, gaze has both advantages and\n",
      "disadvantages. it is a natural mode of input as it is easy\n",
      "to focus on items by looking at them [12, 25]. another\n",
      "advantage is that target acquisition using gaze is very\n",
      "fast, provided the targets are su.ciently large [26].\n",
      "however, gaze is not as accurate as the mouse.\n",
      "inaccuracy originates partly from technological reasons\n",
      "and partly from features of the eye [12]. the size of the\n",
      "fovea and the inability of the remote camera to resolve\n",
      "the fovea position restricts the accuracy of the measured\n",
      "point of gaze to about 0.5 degrees, equivalent to a region\n",
      "spanning approximately 15 pixels on a typical display\n",
      "(17 inch display with a resolution of 1,024 &times; 768 pixels\n",
      "viewed from a distance of 70&nbsp;cm). one problem is\n",
      "drifting; even if a newly calibrated eye tracking device is\n",
      "accurate at first, with continued use the measured point\n",
      "of gaze drifts away from the actual point of gaze. this is\n",
      "partly due to the technology and partly due to the\n",
      "interaction between head movement and eye movement.\n",
      "therefore, the practical accuracy with about 0.5–1 degree of\n",
      "drifting corresponds to about 1–1.5 cm on the screen at\n",
      "a normal viewing distance.\n",
      "<p>\n",
      "\n",
      "when we look at things, we fixate (focus) on them,\n",
      "with fixations typically lasting from 200 to 600 ms [12].\n",
      "for a computer to distinguish whether the user is\n",
      "looking at an object to obtain information or to select it,\n",
      "an interval longer than the typical fixation interval is\n",
      "needed. stampe and reingold [25] used a dwell time (an\n",
      "extended look at the object) of 750&nbsp;ms in their eye typing\n",
      "study. a thousand milliseconds is usually long enough to\n",
      "prevent false selections. for simple tasks, 700&nbsp;ms or less\n",
      "is enough. requiring the user to fixate for long intervals\n",
      "is good for preventing false selections (thus, preventing\n",
      "the so-called midas touch problem [11]), but this is\n",
      "uncomfortable for most users [25].\n",
      "<p>\n",
      "\n",
      "it is important to note that the use of a dwell time\n",
      "criterion for key selection places an upper limit on eye\n",
      "typing speed. in other words, no amount of skill\n",
      "acquisition will allow a user to \"eye press keys\" at a rate\n",
      "faster than 1&nbsp;/&nbsp;<i>t</i><sub>d</sub>, where td is the dwell time. if, for\n",
      "example, <i>t</i><sub>d</sub> = 1,000&nbsp;ms = 1&nbsp;s, the upper limit for typing\n",
      "speed is (60&nbsp;/&nbsp;1)&nbsp;/&nbsp;5 = 12 words per minute (wpm)\n",
      "(following the accepted method in computing typing\n",
      "speed of 1 word = 5 inputted characters).\n",
      "<p>\n",
      "\n",
      "<h3>1.2 typing by gaze</h3>\n",
      "<p>\n",
      "\n",
      "a typical setup has an eye tracker and an on-screen\n",
      "keyboard (see fig.&nbsp;1). the tracking device follows the\n",
      "user’s point of gaze and software records and analyses\n",
      "the gaze behavior. based on the analysis, the system\n",
      "decides which letter the user wants to type.\n",
      "during eye typing, the user first locates the letter by\n",
      "moving his or her gaze (focus) to it. when using dwell\n",
      "time for selection, the user continues to look at the letter\n",
      "for a pre-defined time interval. feedback is typically\n",
      "shown on both focus and selection (for a review, see\n",
      "[16]).\n",
      "<p>\n",
      "\n",
      "<center>\n",
      "<img src=\"uais2006-f1.jpg\"><br>\n",
      "<b>fig.&nbsp;1</b>. on-screen keyboard and eye tracking device\n",
      "</center>\n",
      "<p>\n",
      "\n",
      "<h2>2 \n",
      "introduction</h2>\n",
      "<p>\n",
      "\n",
      "unmanned aerial vehicles (uavs) or “drones” have a long history in military \n",
      "applications. they are able to carry heavy loads over long distances, while \n",
      "being controlled remotely by an operator. however, low-cost drones are now \n",
      "offering many non-military possibilities. these drones are light-weight, fly \n",
      "only a limited time (e.g., &lt; 20 minutes), and have limited range (e.g., &lt; 1 \n",
      "km). for instance, the off-the-shelf a.r. parrot drone costs around $400 and \n",
      "can be controlled from a pc, tablet, or smartphone (figure 1). it has a \n",
      "front-facing camera transmitting live-images to the pilot via wi-fi. people \n",
      "share videos, tips, and new software applications supported by its open-api.\n",
      "<p>\n",
      "\n",
      "<center>\n",
      "<a href=\"etra2014-f1.jpg\"><img src=\"etra2014-f1.jpg\" width=\"300\"></a><br>\n",
      "<b>figure 1.</b>\n",
      "commodity drone (a.r. parrot 2.0). the three\n",
      "main control axes are pitch, roll and yaw.\n",
      "</center><p>\n",
      "\n",
      "this paper considers gaze as a potential input in the development of \n",
      "interfaces for drone piloting. when using gaze input for drone control, \n",
      "spatial awareness is directly conveyed without being mediated through another \n",
      "input device. the pilot sees what the drone sees. however, it is an open \n",
      "research question how best to include gaze in the command of drones. if gaze \n",
      "is too difficult to use, people may crash or lose the drones, which is \n",
      "dangerous and costly.\n",
      "<p>\n",
      "\n",
      "\n",
      "first we present our motivation with examples of use-cases where gaze could \n",
      "offer a significant contribution. then we present previous research within the \n",
      "area. we designed an experiment to get feedback from users on their immediate, \n",
      "first-time impression of gaze-controlled flying. the experiment is presented \n",
      "in section 5. the paper then finishes with general discussion on how best to \n",
      "utilize gaze for drone control.\n",
      "<p>\n",
      "\n",
      "<h2>\n",
      "introduction</h2>\n",
      "<p>\n",
      "\n",
      "many computer users are switching from personal computers to smartphones or \n",
      "tablets for their daily computing needs. this transition has created growth in \n",
      "mobile application development, with many companies offering exclusive services \n",
      "and promotions over their mobile apps.  the goal is to lure users to smartphones. \n",
      "and the rewards are huge.  since launching the app store in 2008, apple's \n",
      "developer community has earned $70 billion worldwide, with $20 billion in 2016 \n",
      "alone [12, 13].  furthermore, gaming is the top-grossing across 25 app categories, \n",
      "with games accounting for 25% of all available apps [20].  this paper focuses on \n",
      "input methods for mobile games.\n",
      "<p>\n",
      "\n",
      "  smartphones receive input through a touchscreen, a camera, microphone, \n",
      "  accelerometer, etc. touch input is the primary input method on smartphones, and \n",
      "  this is also true for mobile games. some drawbacks of touch input for the player \n",
      "  are lack of tactile feedback and occlusion of the display [8]. smartphones and \n",
      "  tablets typically include an accelerometer which measures the tilting motion and \n",
      "  the physical orientation of the device. device orientation can be used for \n",
      "  controlling game objects. such control, commonly called \"tilt\", is widely used \n",
      "  as an input method in mobile games. in addition, a smartphone's camera can track \n",
      "  movement in the real world. using the front-facing camera, we can track the \n",
      "  user's face or eye, and even obtain their facial expression. thus, eye-tracking \n",
      "  and head-tracking are possible input methods for hands-free games, in addition \n",
      "  to tilt.  one benefit of hands-free input is to provide more entertainment \n",
      "  options for people with special needs. however, using the eyes as an input \n",
      "  method is problematic, since the eyes are also our main perception method [16]. \n",
      "  notably, ease of use and novelty has an impact on \"perceived enjoyment\" in \n",
      "  mobile games [18]. \n",
      "<p>\n",
      "\n",
      "  with ios 11, apple introduced arkit, a framework for creating augmented reality \n",
      "  (ar) apps for the <i>iphone</i> and <i>ipad</i>. arkit combined with the truedepth \n",
      "  front-facing camera available on the <i>iphone x</i> enables face or eye tracking in ar \n",
      "  apps. the truedepth camera gives the position, orientation, and expression of \n",
      "  the face in real-time. it also enables face id, the facial recognition method \n",
      "  for authentication on an <i>iphone x</i>. \n",
      "<p>\n",
      "\n",
      "  in this paper, we evaluate the functionality of eye-tracking and head-tracking, \n",
      "  using arkit and the truedepth camera. we compare tilt-input, head-tracking, and \n",
      "  eye-tracking for a custom-designed game called <i>trackmaze</i>. with <i>trackmaze</i>, the \n",
      "  player navigates a virtual ball through a static maze. we address several \n",
      "  questions: is it possible to use eye-tracking and head-tracking for mobile games \n",
      "  that need whole-display navigation? can eye-tracking and head-tracking result in \n",
      "  better perceived enjoyment? can eye-tracking or head-tracking compete with \n",
      "  tilt-input, the natural input method for maze games? which input method is \n",
      "  easiest to use? do users prefer one input method over the others?\n",
      "<p>\n",
      "\n",
      "  in the next section, we review studies on mobile games related to tilt-input, \n",
      "  eye-tracking, and head tracking. then, we discuss the methodology and results of \n",
      "  our user study. we then offer conclusions on user performance and enjoyment for \n",
      "  tilt, eye-tracking, and head-tracking as input methods.\n",
      "<p>\n",
      "\n",
      "<h2>2 \n",
      "introduction</h2>\n",
      "\n",
      "<h3>iso 9241 - part 9</h3>\n",
      "\n",
      "beginning with the apple <i>macintosh</i> in 1984, graphical user interfaces (guis) have \n",
      "evolved and matured. the key feature of modern guis is the ability for users to \n",
      "interact with simple point-and-select operations. the most common pointing device in \n",
      "desktop systems is the mouse. to select an on-screen target with a mouse, a user \n",
      "manipulates the mouse to maneuver the cursor to a target, then selects the target by \n",
      "pressing and releasing a button. simple as this seems, the interaction is even simpler \n",
      "with an eye tracker. the user locates the target by looking at it and follows \n",
      "immediately with selection [10].\n",
      "<p>\n",
      "\n",
      "although considerable research exists in eye tracking [3, 6, 9, 10], none has evaluated \n",
      "eye tracking with <i>iso 9241 ergonomic requirements for office work with visual \n",
      "display terminals (vdts) - part 9: requirements for non-keyboard input devices</i>. \n",
      "iso 9241-9 establishes uniform guidelines and testing procedures for evaluating \n",
      "computer pointing devices. the metric for comparison is <i>throughput</i>, in bits per \n",
      "second (bits/s), which includes both the speed and accuracy of users' performance. \n",
      "the equation for throughput is fitts' index of performance except using an <i>effective</i> \n",
      "index of difficulty (<i>id</i><sub>e</sub>). specifically,\n",
      "<p>\n",
      "\n",
      "<table width=90%>\n",
      "<tr>\n",
      "<td width=10%>\n",
      "<td width=80%>\n",
      "<i>throughput</i> = <i>id</i><sub>e</sub> / <i>mt</i>\n",
      "<td width=10% align=right>\n",
      "(1)\n",
      "</table>\n",
      "<p>\n",
      "\n",
      "where <i>mt</i> is the mean movement time, in seconds, for all trials within the same \n",
      "condition, and\n",
      "<p>\n",
      "\n",
      "<table width=90%>\n",
      "<tr>\n",
      "<td width=10%>\n",
      "<td width=80%>\n",
      "<i>id</i><sub>e</sub> = log<sub>2</sub>(<i>d</i> / <i>w</i><sub>e</sub> + 1).\n",
      "<td width=10% align=right>\n",
      "(2)\n",
      "</table>\n",
      "<p>\n",
      "\n",
      "<i>id</i><sub>e</sub>, in bits, is calculated from <i>d</i>, the distance to the target, and \n",
      "<i>w</i><sub>e</sub>, the effective width \n",
      "of the target. <i>w</i><sub>e</sub> is calculated as\n",
      "<p>\n",
      "\n",
      "<table width=90%>\n",
      "<tr>\n",
      "<td width=10%>\n",
      "<td width=80%>\n",
      "<i>w</i><sub>e</sub> = 4.133 × <i>sd</i>.\n",
      "<td width=10% align=right>\n",
      "(3)\n",
      "</table>\n",
      "<p>\n",
      "\n",
      "where <i>sd</i> is the standard deviation in the selection coordinates measured along the \n",
      "line from the center of the home square to the center of a target. using effective width \n",
      "allows throughput to incorporate the spatial variability in human performance. it \n",
      "includes both speed and accuracy [5].\n",
      "<p>\n",
      "\n",
      "<h3>prior evaluations</h3>\n",
      "\n",
      "iso 9241-9 was in draft international standard form in 1998 and became an \n",
      "international standard in 2000. if one considers mouse evaluations in research not \n",
      "following the standard, throughput ranged from about 2.6 bits/s to 12.5 bits/s. on the \n",
      "contrary, studies conforming to the standard reported throughput from about 3.7 bits/s \n",
      "to 4.9 bits/s [8]. the data appear much more uniform and consistent. in short, \n",
      "iso 9241-9 improves the quality and comparability of device evaluations. \n",
      "<p>\n",
      "\n",
      "although several papers follow iso 9241-9 and dozens of others use fitts' law to \n",
      "evaluate non-keyboard input devices, ware and mikaelian published in 1987 what \n",
      "remains the only fitts' law evaluation of an eye tracking system [10]. they used a \n",
      "serial fitts' law task to test three eye tracking techniques. task completion time was \n",
      "the only performance measure used. they compared eye tracking with the mouse but \n",
      "did not calculate or report on throughput as a performance measure. no eye tracking \n",
      "evaluation paper has ever been published since then using fitts' law (or iso 9241-9). \n",
      "<p>\n",
      "\n",
      "by following the standard and comparing throughput for eye tracking with a baseline \n",
      "technique (i.e., a mouse), we can determine how good an eye tracking system is. this \n",
      "paper is the first eye tracking evaluation conforming to iso 9241-9.\n",
      "<p>\n",
      "\n",
      "the rest of this paper is organized as follows. in section 2, we described the \n",
      "methodology of our experiment. in section 3, the results are presented and discussed. \n",
      "finally, we present our conclusions in section 4.\n",
      "<p>\n"
     ]
    }
   ],
   "source": [
    "doc1 = doc1.lower()\n",
    "doc2 = doc2.lower()\n",
    "doc3 = doc3.lower()\n",
    "doc4 = doc4.lower()\n",
    "doc5 = doc5.lower()\n",
    "\n",
    "title_d1 = doc1[doc1.find('<title>'):doc1.find('</title>')]\n",
    "print(title_d1)\n",
    "\n",
    "title_d2 = doc2[doc2.find('<title>'):doc2.find('</title>')]\n",
    "print(title_d2)\n",
    "\n",
    "title_d3 = doc3[doc3.find('<title>'):doc3.find('</title>')]\n",
    "print(title_d3)\n",
    "\n",
    "title_d4 = doc4[doc4.find('<title>'):doc4.find('</title>')]\n",
    "print(title_d4)\n",
    "\n",
    "title_d5 = doc5[doc5.find('<title>'):doc5.find('</title>')]\n",
    "print(title_d5)\n",
    "\n",
    "abstract_d1 = doc1[doc1.find('abstract'):doc1.find('keywords')]\n",
    "print(abstract_d1)\n",
    "\n",
    "abstract_d2 = doc2[doc2.find('abstract'):doc2.find('keywords')]\n",
    "print(abstract_d2)\n",
    "\n",
    "abstract_d3 = doc3[doc3.find('abstract'):doc3.find('keywords')]\n",
    "print(abstract_d3)\n",
    "\n",
    "abstract_d4 = doc4[doc4.find('abstract'):doc4.find('keywords')]\n",
    "print(abstract_d4)\n",
    "\n",
    "abstract_d5 = doc5[doc5.find('abstract'):doc5.find('keywords')]\n",
    "print(abstract_d5)\n",
    "\n",
    "intro_d1 = doc1[doc1.find('introduction'):doc1.find('corrected-to-normal')-125]\n",
    "print(intro_d1)\n",
    "\n",
    "intro_d2 = doc2[doc2.find('introduction'):doc2.find('previous research')]\n",
    "print(intro_d2)\n",
    "\n",
    "intro_d3 = doc3[doc3.find('introduction'):doc3.find('?')-71]\n",
    "print(intro_d3)\n",
    "\n",
    "intro_d4 = doc4[doc4.find('introduction'):doc4.find('related work')]\n",
    "print(intro_d4)\n",
    "\n",
    "intro_d5 = doc5[doc5.find('introduction'):doc5.find('methodology')+140]\n",
    "print(intro_d5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "5\n",
      "9\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "0\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "6\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "8\n",
      "0\n",
      "0\n",
      "20\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "15\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "#doc1\n",
    "a = Counter(title_d1.split())\n",
    "print(a['auditory'])\n",
    "print(a['and'])\n",
    "print(a['visual'])\n",
    "\n",
    "b = Counter(abstract_d1.split())\n",
    "print(b['auditory'])\n",
    "print(b['and'])\n",
    "print(b['visual'])\n",
    "\n",
    "c = Counter(intro_d1.split())\n",
    "print(c['auditory'])\n",
    "print(c['and'])\n",
    "print(c['visual'])\n",
    "\n",
    "#doc2\n",
    "a = Counter(title_d2.split())\n",
    "print(a['auditory'])\n",
    "print(a['and'])\n",
    "print(a['visual'])\n",
    "\n",
    "b = Counter(abstract_d2.split())\n",
    "print(b['auditory'])\n",
    "print(b['and'])\n",
    "print(b['visual'])\n",
    "\n",
    "c = Counter(intro_d2.split())\n",
    "print(c['auditory'])\n",
    "print(c['and'])\n",
    "print(c['visual'])\n",
    "\n",
    "#doc3\n",
    "a = Counter(title_d3.split())\n",
    "print(a['auditory'])\n",
    "print(a['and'])\n",
    "print(a['visual'])\n",
    "\n",
    "b = Counter(abstract_d3.split())\n",
    "print(b['auditory'])\n",
    "print(b['and'])\n",
    "print(b['visual'])\n",
    "\n",
    "c = Counter(intro_d3.split())\n",
    "print(c['auditory'])\n",
    "print(c['and'])\n",
    "print(c['visual'])\n",
    "\n",
    "#doc4\n",
    "a = Counter(title_d4.split())\n",
    "print(a['auditory'])\n",
    "print(a['and'])\n",
    "print(a['visual'])\n",
    "\n",
    "b = Counter(abstract_d4.split())\n",
    "print(b['auditory'])\n",
    "print(b['and'])\n",
    "print(b['visual'])\n",
    "\n",
    "c = Counter(intro_d4.split())\n",
    "print(c['auditory'])\n",
    "print(c['and'])\n",
    "print(c['visual'])\n",
    "\n",
    "#doc5\n",
    "a = Counter(title_d5.split())\n",
    "print(a['auditory'])\n",
    "print(a['and'])\n",
    "print(a['visual'])\n",
    "\n",
    "b = Counter(abstract_d5.split())\n",
    "print(b['auditory'])\n",
    "print(b['and'])\n",
    "print(b['visual'])\n",
    "\n",
    "c = Counter(intro_d5.split())\n",
    "print(c['auditory'])\n",
    "print(c['and'])\n",
    "print(c['visual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n",
      "Yes\n",
      "No\n",
      "Yes\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "if(title_d1.find('eye') != -1 and abstract_d1.find('performance') != -1 and intro_d1.find('method') != -1):\n",
    "    print('Yes')\n",
    "else:\n",
    "    print('No')\n",
    "    \n",
    "if(title_d2.find('eye') != -1 and abstract_d2.find('performance') != -1 and intro_d2.find('method') != -1):\n",
    "    print('Yes')\n",
    "else:\n",
    "    print('No')\n",
    "\n",
    "if(title_d3.find('eye') != -1 and abstract_d3.find('performance') != -1 and intro_d3.find('method') != -1):\n",
    "    print('Yes')\n",
    "else:\n",
    "    print('No')\n",
    "    \n",
    "if(title_d4.find('eye') != -1 and abstract_d4.find('performance') != -1 and intro_d4.find('method') != -1):\n",
    "    print('Yes')\n",
    "else:\n",
    "    print('No')\n",
    "    \n",
    "if(title_d5.find('eye') != -1 and abstract_d5.find('performance') != -1 and intro_d5.find('method') != -1):\n",
    "    print('Yes')\n",
    "else:\n",
    "    print('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
