{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://www.yorku.ca/mack/chi03d.html'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('Document1.html', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\">\\n<html>\\n<title>\\nAuditory and Visual Feedback During Eye Typing\\n</title>\\n\\n<body>\\n<blockquote>\\n<hr>\\n<font size=-1>\\nMajaranta, P., MacKenzie, I. S., Aula, A., & Räihä, K.-J.\\n(2003). \\nAuditory and visual feedback during eye typing.\\n<i>Proceedings of the ACM Conference on Human Factors in Computing Systems - CHI 2003</i>, pp. 766-767.\\nNew York: ACM.\\n</font>\\n<hr>\\n\\n<center>\\n<h1>\\nAuditory and Visual Feedback During Eye Typing\\n</h1>\\n\\n<h3>\\nMajaranta, P., \\n<a href=\"http://www.yorku.ca/mack/\">MacKenzie, I. S.</a>, \\nAula, A., & \\n<a href=\"http://www.cs.uta.fi/~kjr/\">Räihä, K.-J.</a>\\n</h3>\\n\\n\\nUnit for Computer-Human Interaction (TAUCHI)<br>\\nDepartment of Computer and Information Sciences <br>\\nFIN-33014 University of Tampere, Finland<br>\\n{curly, scott, aula, kjr}@cs.uta.fi<br>\\n</center>\\n\\n<blockquote>\\n<b>ABSTRACT</b><br>\\nWe describe a study on how auditory and visual feedback affects eye typing. Results show that the feedback method influences both text entry speed and error rate. In addition, a proper feedback mode facilitates eye typing by reducing the user\\'s need to switch her gaze between the on-screen keyboard and the typed text field.\\n<p>\\n\\n<b>Keywords</b><br>\\nEye typing, text entry, feedback modalities, disabled users\\n\\n</blockquote>\\n\\n<h2>INTRODUCTION\\n</h2>\\n<p>\\nFor people with severe disabilities their eyes may be the only means for communication. Even though eye typing has been studied for many years, there is little research on design issues [2]. Our goal was to study how feedback could facilitate the tedious [1] eye typing task and make gaze-based computer-aided communication more practical for those who need it.\\n<p>\\n\\n<h3>\\nFeedback Modes\\n</h3>\\n\\n<p>\\nDuring eye typing the user first focuses on the desired letter. To select the focused letter she continues to fixate on it thus using dwell time as an activation command. Feedback is given for focus and selection. The following four feedback modes were tested. \\n<p>\\n\\n<b>Visual only.</b> In the Visual only mode, the key is highlighted on focus (the 2nd key on left in Figure 1) and its symbol shrinks as dwell time elapses. The shrinking draws the attention in, helping the user focus on the center of the key. On selection the letter turns red and the key goes down. \\n<p>\\n\\n<center>\\n<img src=\"chi03d-f1.gif\"><br>\\n<b>Figure 1.</b> Animation for Visual only feedback mode \\n</center>\\n<p>\\n\\n<b>Speech only.</b> The Speech only mode did not use visual feedback. The symbol on the key was spoken on selection.\\n<p>\\n\\n<b>Click plus visual.</b> The Click plus visual mode uses two modalities; it has the same visual feedback seen in Figure 1 and, in addition, a short \"click\" is heard on selection.\\n<p>\\n\\n<b>Speech plus visual.</b> The Speech plus visual mode again uses the same visual feedback plus the symbol on the key is spoken on selection.\\n<p>\\nThe dwell time was the same for all modes: 400 ms before the start of the focus and 900 ms before selection. A summary of the modes follows.\\n\\n<center>\\n<table bgcolor=\"#f0f0f0\" border=1 cellpadding=5 cellspacing=0>\\n<tr bgcolor=\"#e0e0e0\">\\n<th align=center>Feedback mode\\n<th align=center>While focused\\n<th align=center>When selected\\n\\n<tr>\\n<td align=left>Visual only\\n<td align=left>shrinking letter\\n<td align=left>red letter, key down\\n\\n<tr>\\n<td align=left>Speech only\\n<td align=left>none\\n<td align=left>letter spoken\\n\\n<tr>\\n<td align=left>Click + visual\\n<td align=left>shrinking letter\\n<td align=left>red letter, key down, click\\n\\n<tr>\\n<td align=left>Speech + visual\\n<td align=left>shrinking letter\\n<td align=left>red letter, key down,<br> \\nletter spoken\\n</table>\\n</center>\\n\\n<h2>\\nMETHOD\\n</h2>\\n<p>\\nOur study used 13 participants (5 females, 8 males, mean age 23 years). All were able-bodied with normal or corrected-to-normal vision. None had previous experience with eye tracking or eye typing but all were familiar with desktop computers.\\n<p>\\nThe setup combined two computers and an eye tracking device (SensoMotoric Instruments iView X RED-III, 50 Hz sampling, 1-degree gaze position accuracy, see Figure 2).  \\n<p>\\n\\n<center>\\n<img src=\"chi03d-f2.gif\"><br>\\n<b>Figure 2.</b> SMI eye tracker and experimental software\\n</center>\\n\\n<p>\\nThe software had an on-screen keyboard, a typed text field above, and a source text field below. The user first read the source text and then eye typed it letter by letter by gazing at a letter for the predefined dwell time. The typed text appeared in the upper field. The qwerty layout was chosen over alternatives based on pilot users\\' comments. For the experiment, a special \"ready\" key was added. Activating this key cleared the typed text field and loaded a new source sentence.\\n<p>\\nWe collected three types of data: fixation data, raw eye data, and event data logged by the experimental software. We also videotaped all trials.\\n<p>\\nThe experiment was a 4 x 4 repeated measures design with 4 feedback modes and 4 blocks of sentences. The order of administering the feedback modes was randomized across blocks and participants to minimize asymmetric learning effects. Each block involved the entry of the same five short phrases of text. The user was instructed to memorize the source sentence and then eye type it as fast and accurately as possible. There was a short pause after each block. Each participant came to the test four times. In the last visit we interviewed the user and gave a questionnaire. The total number of phrases was 1040 (13 participants x 4 feedback modes x 4 blocks x 5 sentences).\\n<p>\\n\\n<h2>\\nRESULTS AND DISCUSSION\\n</h2>\\n<p>\\nThe grand mean for entry speed was 6.97 words per minute. This is typical for eye typing [1, 2], but it is still too low for fluent text entry. As evident in Figure 3, participants improved with practice, as a significant main effect for block was found \\n(<i>F</i><sub>3,36</sub> = 10.92, <i>p</i> &lt; .0001). \\n<p>\\n\\n<center>\\n<img src=\"chi03d-f3.gif\"><br>\\n<b>Figure 3.</b> Entry speed (wpm) by feedback mode and block\\n</center>\\n\\n\\n<p>\\nThe main effect for feedback mode was also significant \\n(<i>F</i><sub>3,36</sub> = 8.77, <i>p</i> &lt; .0005). \\nOverall, the combined use of Click plus visual feedback yielded the fastest entry rate, with participants achieving a fourth block mean of 7.55 wpm. The other fourth-block means were 7.14 wpm (Speech plus visual), 7.12 wpm (Visual only), and 7.00 wpm (Speech only). The dwell time was constant for all feedback modes. The entry rate will naturally speed up with a shorter dwell duration. This may be possible as a user develops proficiency with the apparatus.\\n<p>\\nParticipants\\' accuracy also improved significantly with practice \\n(<i>F</i><sub>3,36</sub> = .09, <i>p</i> = .005), \\nas seen in Figure 4. Character-level error rates were quite low overall with a grand mean of 0.54%. Participants proceeded quite cautiously to avoid a loss of calibration, which occurred occasionally and necessitated re-typing a phrase. There were significant main effects for feedback mode \\n(<i>F</i><sub>3,36</sub> = 5.01, <i>p</i> = .005). \\nEye typing with Speech only feedback was the most accurate technique throughout the experiment with error rates under 0.8% on all four blocks.\\n<p>\\n\\n<center>\\n<img src=\"chi03d-f4.gif\"><br>\\n<b>Figure 4.</b> Error rate (%) by feedback mode and block\\n</center>\\n<p>\\nOur experimental software logged various events of interest. One such event was \"read text\", referring to a participant switching their point of gaze to the typed text field to review the text typed so far. The values analyzed were the mean number of such events per phrase of text entered.\\n<p>\\nThe overall mean was 1.63 read text events per phrase. By feedback mode, the means were 1.17 (Speech only), 1.28 (Click plus visual), 1.24 (Speed plus visual), and 2.77 (Visual only). The mean for Visual only feedback mode was significantly higher than for the other modes \\n(<i>F</i><sub>3,36</sub> = 30.06, <i>p</i> &lt; .0001). \\nThe users\\' gaze behavior shows (Figure 5) that auditory feedback (click or spoken) significantly reduces the need to review and verify the typed text.\\n<p>\\n\\n<center>\\n<img src=\"chi03d-f5.gif\"><br>\\n<b>Figure 5.</b> Read text events (mean per phrase) by feedback mode and block\\n</center>\\n<p>\\n\\n<h2>\\nCONCLUSIONS AND FUTURE WORK\\n</h2>\\n<p>\\nOur results show that the feedback mode affects typing speed, error rate, and the user\\'s gaze behavior during eye typing. In particular, proper feedback may dramatically reduce the need to switch gaze between the soft keyboard and the typed text field thus reducing the entry time. The results also suggest that auditory feedback (click or spoken) is a more effective indication of selection than visual feedback alone. \\n<p>\\nThe data analysis is ongoing; there are other interactions to analyze, such as gaze path, the types of errors, and the results of the questionnaire.\\n<p>\\n\\n<h2>\\nREFERENCES\\n</h2>\\n<p>\\n1. Frey, L.A, White, K.P. Jr., and Hutchinson, T.E. Eye-gaze word processing, <i>IEEE Transactions on Systems, Man, and Cybernetics 20</i> (4), 1990, 944-950.\\n<p>\\n2. Majaranta, P., and Räihä, K.-J. Twenty years of eye typing: Systems and design issues, <i>Proceedings of ETRA\\'02</i>, New Orleans, LA, ACM Press, 2002, 15-22.\\n<p>\\n\\n<p>\\n</blockquote>\\n<p>\\n</body>\\n<p>\\n</html>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = open('Document1.html')\n",
    "d1 = d1.read()\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auditory and visual feedback during eye typing\n",
      "we describe a study on how auditory and visual feedback affects eye typing. results show that the feedback method influences both text entry speed and error rate. in addition, a proper feedback mode facilitates eye typing by reducing the user's need to switch her gaze between the on-screen keyboard and the typed text field.\n",
      "\n",
      "during eye typing the user first focuses on the desired letter. to select the focused letter she continues to fixate on it thus using dwell time as an activation and. feedback is given for focus and selection. the following four feedback modes were tested. \n",
      "<p>\n",
      "\n",
      " visual only  in the visual only mode the key is highlighted on focus (the 2nd key on left in figure 1) and its symbol shrinks as dwell time elapses. the shrinking draws the attention in helping the user focus on the center of the key. on selection the letter turns red and the key goes down. \n",
      " the speech only mode did not use visual feedback. the symbol on the key was spoken on selection.\n",
      "<p>\n",
      "\n",
      " click plus visual  the click plus visual mode uses two modalities; it has the same visual feedback seen in figure 1 and in addition a short \"click\" is heard on selection.\n",
      "<p>\n",
      "\n",
      " speech plus visual  the speech plus visual mode again uses the same visual feedback plus the symbol on the key is spoken on selection.\n",
      "<p>\n",
      "the dwell time was the same for all modes: 400 ms before the start of the focus and 900 ms before selection. a summary of the modes follows.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "d1 = d1.lower()\n",
    "res = re.search('<title>\\n(.*)\\n</title>',d1)\n",
    "abst = re.search('</b><br>\\n(.*)\\n<p>',d1)\n",
    "intro = re.search('<p>\\n(.*)\\n<p>',d1)\n",
    "title = d1[(d1.index('<title>\\n')+len('<title>\\n')):(d1.index('\\n</title>'))]\n",
    "abstract = d1[(d1.index('</b><br>\\n')+len('</b><br>\\n')):(d1.index('\\n<p>'))]\n",
    "intro1 = d1[(d1.index('<p>\\n')+len('<p>\\n')):(d1.index('\\n<p>'))] \n",
    "intro2 = d1[(d1.index('\\n<p>\\n\\n<h3>\\nfeedback modes\\n</h3>\\n\\n<p>\\n')+len('\\n<p>\\n\\n<h3>\\nfeedback modes\\n</h3>\\n\\n<p>\\n')):(d1.index('\\n<p>\\n\\n<center>\\n<img'))]  \n",
    "intro3 = d1[(d1.index('\\n</center>\\n<p>\\n\\n<b>speech only.</b>')+len('\\n</center>\\n<p>\\n\\n<b>speech only.</b>')):(d1.index('\\n\\n<center>\\n<table'))]\n",
    "\n",
    "introduction = intro1+\"\\n\"+intro2+\"\\n\"+intro3\n",
    "introduction = introduction.replace(\"<b>\",\" \")\n",
    "introduction = introduction.replace(\".</b>\",\" \")\n",
    "introduction = introduction.replace(\",\",\"\")\n",
    "introduction = introduction.replace(\"command\",\"and\")\n",
    "\n",
    "print(title)\n",
    "print(abstract)\n",
    "print(introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effects of feedback and dwell time on eye typing speed and accuracy\n",
      "\n",
      " eye typing provides a means of communication\n",
      "that is especially useful for people with disabilities.\n",
      "however, most related research addresses technical issues\n",
      "in eye typing systems, and largely ignores design\n",
      "issues. this paper reports experiments studying the impact\n",
      "of auditory and visual feedback on user performance\n",
      "and experience. results show that feedback\n",
      "impacts typing speed, accuracy, gaze behavior, and\n",
      "subjective experience. also, the feedback should be\n",
      "matched with the dwell time. short dwell times require\n",
      "simplified feedback to support the typing rhythm,\n",
      "whereas long dwell times allow extra information on the\n",
      "eye typing process. both short and long dwell times\n",
      "benefit from combined visual and auditory feedback. six\n",
      "guidelines for designing feedback for gaze-based text\n",
      "entry are provided\n",
      "\n",
      "eye typing refers to the production of text using the\n",
      "focus of the eye (aka gaze) as a means of input. it is\n",
      "especially needed by people with severe disabilities,\n",
      "where controlling the eyes is sometimes the only means\n",
      "of interaction with the world. their need for an effective\n",
      "communication system is acute.\n",
      "<p>\n",
      "\n",
      "research on technical aspects of eye typing extends\n",
      "over 20 years. however, there is little research on design\n",
      "issues [16]. the authors’ work [17, 18, 19] is an attempt\n",
      "to partly fill this gap. such work investigated how feedback\n",
      "can facilitate the tedious [5] eye typing task. this\n",
      "paper summarizes the results of three experiments\n",
      "studying various aspects of feedback during eye typing.\n",
      "because gaze has some unique features, the paper\n",
      "briefly introduces gaze as an input method. this is followed\n",
      "by a description of eye typing. examples of the\n",
      "most relevant research on eye gaze feedback are reviewed.\n",
      "the methods and results of the experiments are\n",
      "then presented, followed by guidelines gleaned from the\n",
      "results.\n",
      "<p>\n",
      "\n",
      "<h3>1.1 gaze input</h3>\n",
      "<p>\n",
      "\n",
      "gaze is naturally used to obtain visual information. for\n",
      "example, gaze location shows the focus of attention [13].\n",
      "as an input method, gaze has both advantages and\n",
      "disadvantages. it is a natural mode of input as it is easy\n",
      "to focus on items by looking at them [12, 25]. another\n",
      "advantage is that target acquisition using gaze is very\n",
      "fast, provided the targets are su.ciently large [26].\n",
      "however, gaze is not as accurate as the mouse.\n",
      "inaccuracy originates partly from technological reasons\n",
      "and partly from features of the eye [12]. the size of the\n",
      "fovea and the inability of the remote camera to resolve\n",
      "the fovea position restricts the accuracy of the measured\n",
      "point of gaze to about 0.5 degrees, equivalent to a region\n",
      "spanning approximately 15 pixels on a typical display\n",
      "(17 inch display with a resolution of 1,024 &times; 768 pixels\n",
      "viewed from a distance of 70&nbsp;cm). one problem is\n",
      "drifting; even if a newly calibrated eye tracking device is\n",
      "accurate at first, with continued use the measured point\n",
      "of gaze drifts away from the actual point of gaze. this is\n",
      "partly due to the technology and partly due to the\n",
      "interaction between head movement and eye movement.\n",
      "therefore, the practical accuracy with about 0.5–1 degree of\n",
      "drifting corresponds to about 1–1.5 cm on the screen at\n",
      "a normal viewing distance.\n",
      "<p>\n",
      "\n",
      "when we look at things, we fixate (focus) on them,\n",
      "with fixations typically lasting from 200 to 600 ms [12].\n",
      "for a computer to distinguish whether the user is\n",
      "looking at an object to obtain information or to select it,\n",
      "an interval longer than the typical fixation interval is\n",
      "needed. stampe and reingold [25] used a dwell time (an\n",
      "extended look at the object) of 750&nbsp;ms in their eye typing\n",
      "study. a thousand milliseconds is usually long enough to\n",
      "prevent false selections. for simple tasks, 700&nbsp;ms or less\n",
      "is enough. requiring the user to fixate for long intervals\n",
      "is good for preventing false selections (thus, preventing\n",
      "the so-called midas touch problem [11]), but this is\n",
      "uncomfortable for most users [25].\n",
      "<p>\n",
      "\n",
      "it is important to note that the use of a dwell time\n",
      "criterion for key selection places an upper limit on eye\n",
      "typing speed. in other words, no amount of skill\n",
      "acquisition will allow a user to \"eye press keys\" at a rate\n",
      "faster than 1&nbsp;/&nbsp;<i>t</i><sub>d</sub>, where td is the dwell time. if, for\n",
      "example, <i>t</i><sub>d</sub> = 1,000&nbsp;ms = 1&nbsp;s, the upper limit for typing\n",
      "speed is (60&nbsp;/&nbsp;1)&nbsp;/&nbsp;5 = 12 words per minute (wpm)\n",
      "(following the accepted method in computing typing\n",
      "speed of 1 word = 5 inputted characters).\n",
      "<p>\n",
      "\n",
      "<h3>1.2 typing by gaze</h3>\n",
      "<p>\n",
      "\n",
      "a typical setup has an eye tracker and an on-screen\n",
      "keyboard (see fig.&nbsp;1). the tracking device follows the\n",
      "user’s point of gaze and software records and analyses\n",
      "the gaze behavior. based on the analysis, the system\n",
      "decides which letter the user wants to type.\n",
      "during eye typing, the user first locates the letter by\n",
      "moving his or her gaze (focus) to it. when using dwell\n",
      "time for selection, the user continues to look at the letter\n",
      "for a pre-defined time interval. feedback is typically\n",
      "shown on both focus and selection (for a review, see\n",
      "[16]).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Document 2\n",
    "url = 'https://www.yorku.ca/mack/uais2006.html'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('Document2.html', 'wb').write(r.content)\n",
    "d2 = open('Document2.html')\n",
    "d2 = d2.read()\n",
    "d2 = d2.lower()\n",
    "\n",
    "title2 = d2[(d2.index('<title>\\n')+len('<title>\\n')):(d2.index('\\n</title>'))]\n",
    "abstract2 = d2[(d2.index('</b>')+len('</b>')):(d2.index('entry are provided')+len('entry are provided'))]\n",
    "introduction2 = d2[(d2.index('introduction</h2>\\n<p>\\n\\n')+len('introduction</h2>\\n<p>\\n\\n')):(d2.index('\\n<p>\\n\\n<center>\\n<img'))]\n",
    "\n",
    "print(title2+\"\\n\")\n",
    "print(abstract2+\"\\n\")\n",
    "print(introduction2+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the use of gaze to control drones\n",
      "\n",
      "this paper presents an experimental investigation of gaze-based control modes \n",
      "for unmanned aerial vehicles (uavs or “drones”). ten participants performed a \n",
      "simple flying task. we gathered empirical measures, including task completion \n",
      "time, and examined the user experience for difficulty, reliability, and fun. \n",
      "four control modes were tested, with each mode applying a combination of x-y \n",
      "gaze movement and manual (keyboard) input to control speed (pitch), altitude, \n",
      "rotation (yaw), and drafting (roll). participants had similar task completion \n",
      "times for all four control modes, but one combination was considered \n",
      "significantly more reliable than the others. we discuss design and performance \n",
      "issues for the gaze-plus-manual split of controls when drones are operated \n",
      "using gaze in conjunction with tablets, near-eye displays (glasses), or \n",
      "monitors\n",
      "\n",
      "unmanned aerial vehicles (uavs) or “drones” have a long history in military \n",
      "applications. they are able to carry heavy loads over long distances, while \n",
      "being controlled remotely by an operator. however, low-cost drones are now \n",
      "offering many non-military possibilities. these drones are light-weight, fly \n",
      "only a limited time (e.g., &lt; 20 minutes), and have limited range (e.g., &lt; 1 \n",
      "km). for instance, the off-the-shelf a.r. parrot drone costs around $400 and \n",
      "can be controlled from a pc, tablet, or smartphone (figure 1). it has a \n",
      "front-facing camera transmitting live-images to the pilot via wi-fi. people \n",
      "share videos, tips, and new software applications supported by its open-api\n",
      " as a potential input in the development of \n",
      "interfaces for drone piloting. when using gaze input for drone control, \n",
      "spatial awareness is directly conveyed without being mediated through another \n",
      "input device. the pilot sees what the drone sees. however, it is an open \n",
      "research question how best to include gaze in the command of drones. if gaze \n",
      "is too difficult to use, people may crash or lose the drones, which is \n",
      "dangerous and costly.\n",
      "<p>\n",
      "\n",
      "\n",
      "first we present our motivation with examples of use-cases where gaze could \n",
      "offer a significant contribution. then we present previous research within the \n",
      "area. we designed an experiment to get feedback from users on their immediate, \n",
      "first-time impression of gaze-controlled flying. the experiment is presented \n",
      "in section 5. the paper then finishes with general discussion on how best to \n",
      "utilize gaze for drone control\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Document3\n",
    "\n",
    "url = 'https://www.yorku.ca/mack/etra2014.html'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('Document3.html', 'wb').write(r.content)\n",
    "d3 = open('Document3.html')\n",
    "d3 = d3.read()\n",
    "d3 = d3.lower()\n",
    "\n",
    "title3 = d3[(d3.index('<title>')+len('<title>')):(d3.index('\\n</title>'))]\n",
    "abstract3 = d3[(d3.index('<b>abstract</b><br>\\n')+len('<b>abstract</b><br>\\n')):(d3.index('or \\nmonitors')+len('or \\nmonitors'))]\n",
    "introduction31 = d3[(d3.index('introduction</h2>\\n<p>\\n\\n')+len('introduction</h2>\\n<p>\\n\\n')):(d3.index('open-api')+len('open-api'))]\n",
    "introduction32 = d3[(d3.index('this paper considers gaze')+len('this paper considers gaze')):(d3.index('gaze for drone control')+len('gaze for drone control'))]\n",
    "introduction3 = introduction31+\"\\n\"+introduction32\n",
    "\n",
    "print(title3+\"\\n\")\n",
    "print(abstract3+\"\\n\")\n",
    "print(introduction3+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trackmaze: a comparison of head-tracking, eye-tracking, and tilt as input \n",
      "methods for mobile games\n",
      "\n",
      ". a user study was performed to compare three input methods (tilt, \n",
      "eye-tracking, head-tracking) with two gain levels (low, high) on a custom-made \n",
      "<i>trackmaze</i> mobile game. the task involved maneuvering a virtual ball through a maze \n",
      "while trying to avoid walls. the game was developed in swift using the arkit \n",
      "framework. the truedepth front-facing camera of an apple <i>iphone x</i> was used for the \n",
      "eye-tracking and head-tracking conditions. we evaluated user performance (maze \n",
      "completion time, number of wall hits) and qualitative measures (ease of use, \n",
      "enjoyment, fatigue). tilt input showed the best performance and eye-tracking \n",
      "showed the worst performance. the mean maze completion time was 12.3 seconds for \n",
      "tilt, 22.5 seconds for head-tracking, and 31.8 seconds for eye-tracking. high gain \n",
      "was 26% faster than low gain. tilt was the most precise input method with only \n",
      "1.06 wall hits per trial, compared to head-tracking (2.30) and eye-tracking \n",
      "(4.00). participants preferred tilt and head-tracking over eye-tracking and noted \n",
      "that the eye-tracking interface was fatiguing and hard to use\n",
      "\n",
      "many computer users are switching from personal computers to smartphones or \n",
      "tablets for their daily computing needs. this transition has created growth in \n",
      "mobile application development, with many companies offering exclusive services \n",
      "and promotions over their mobile apps.  the goal is to lure users to smartphones. \n",
      "and the rewards are huge.  since launching the app store in 2008, apple's \n",
      "developer community has earned $70 billion worldwide, with $20 billion in 2016 \n",
      "alone [12, 13].  furthermore, gaming is the top-grossing across 25 app categories, \n",
      "with games accounting for 25% of all available apps [20].  this paper focuses on \n",
      "input methods for mobile games.\n",
      "<p>\n",
      "\n",
      "  smartphones receive input through a touchscreen, a camera, microphone, \n",
      "  accelerometer, etc. touch input is the primary input method on smartphones, and \n",
      "  this is also true for mobile games. some drawbacks of touch input for the player \n",
      "  are lack of tactile feedback and occlusion of the display [8]. smartphones and \n",
      "  tablets typically include an accelerometer which measures the tilting motion and \n",
      "  the physical orientation of the device. device orientation can be used for \n",
      "  controlling game objects. such control, commonly called \"tilt\", is widely used \n",
      "  as an input method in mobile games. in addition, a smartphone's camera can track \n",
      "  movement in the real world. using the front-facing camera, we can track the \n",
      "  user's face or eye, and even obtain their facial expression. thus, eye-tracking \n",
      "  and head-tracking are possible input methods for hands-free games, in addition \n",
      "  to tilt.  one benefit of hands-free input is to provide more entertainment \n",
      "  options for people with special needs. however, using the eyes as an input \n",
      "  method is problematic, since the eyes are also our main perception method [16]. \n",
      "  notably, ease of use and novelty has an impact on \"perceived enjoyment\" in \n",
      "  mobile games [18]. \n",
      "<p>\n",
      "\n",
      "  with ios 11, apple introduced arkit, a framework for creating augmented reality \n",
      "  (ar) apps for the <i>iphone</i> and <i>ipad</i>. arkit combined with the truedepth \n",
      "  front-facing camera available on the <i>iphone x</i> enables face or eye tracking in ar \n",
      "  apps. the truedepth camera gives the position, orientation, and expression of \n",
      "  the face in real-time. it also enables face id, the facial recognition method \n",
      "  for authentication on an <i>iphone x</i>. \n",
      "<p>\n",
      "\n",
      "  in this paper, we evaluate the functionality of eye-tracking and head-tracking, \n",
      "  using arkit and the truedepth camera. we compare tilt-input, head-tracking, and \n",
      "  eye-tracking for a custom-designed game called <i>trackmaze</i>. with <i>trackmaze</i>, the \n",
      "  player navigates a virtual ball through a static maze. we address several \n",
      "  questions: is it possible to use eye-tracking and head-tracking for mobile games \n",
      "  that need whole-display navigation? can eye-tracking and head-tracking result in \n",
      "  better perceived enjoyment? can eye-tracking or head-tracking compete with \n",
      "  tilt-input, the natural input method for maze games? which input method is \n",
      "  easiest to use? do users prefer one input method over the others?\n",
      "<p>\n",
      "\n",
      "  in the next section, we review studies on mobile games related to tilt-input, \n",
      "  eye-tracking, and head tracking. then, we discuss the methodology and results of \n",
      "  our user study. we then offer conclusions on user performance and enjoyment for \n",
      "  tilt, eye-tracking, and head-tracking as input methods.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Document4\n",
    "\n",
    "url = 'https://www.yorku.ca/mack/hcii2018b.html'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('Document4.html', 'wb').write(r.content)\n",
    "d4 = open('Document4.html')\n",
    "d4 = d4.read()\n",
    "d4 = d4.lower()\n",
    "\n",
    "title4 = d4[(d4.index('<title>')+len('<title>')):(d4.index('\\n</title>'))]\n",
    "abstract4 = d4[(d4.index('<b>abstract</b>')+len('<b>abstract</b>')):(d4.index('hard to use')+len('hard to use'))]\n",
    "introduction4 = d4[(d4.index('introduction</h2>\\n<p>\\n\\n')+len('introduction</h2>\\n<p>\\n\\n')):(d4.index('\\n<p>\\n\\n<h2>2'))]\n",
    "\n",
    "\n",
    "print(title4+\"\\n\")\n",
    "print(abstract4+\"\\n\")\n",
    "print(introduction4+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluating eye tracking with iso 9241 - part 9\n",
      "\n",
      "the iso 9241-9 standard for computer pointing devices proposes an \n",
      "evaluation of performance and comfort [4]. this paper is the first eye tracking \n",
      "evaluation conforming to iso 9241-9. we evaluated three techniques and \n",
      "compared them with a standard mouse. the evaluation used throughput (in \n",
      "bits/s) as a measurement of user performance in a multi-directional point-select \n",
      "task. the \"eye tracking long\" technique required participants to look at an \n",
      "on-screen target and dwell on it for 750 ms for selection. results revealed a lower \n",
      "throughput than for the \"eye tracking short\" technique with a 500 ms dwell \n",
      "time. the \"eye+spacebar\" technique allowed participants to \"point\" with the \n",
      "eye and \"select\" by pressing the spacebar upon fixation. this eliminated the \n",
      "need to wait for selection. it was the best among the three eye tracking \n",
      "techniques with a throughput of 3.78 bits/s, which was close to the 4.68 bits/s \n",
      "for the mouse.\n",
      "\n",
      "\n",
      "\n",
      "<h3>iso 9241 - part 9</h3>\n",
      "\n",
      "beginning with the apple <i>macintosh</i> in 1984, graphical user interfaces (guis) have \n",
      "evolved and matured. the key feature of modern guis is the ability for users to \n",
      "interact with simple point-and-select operations. the most common pointing device in \n",
      "desktop systems is the mouse. to select an on-screen target with a mouse, a user \n",
      "manipulates the mouse to maneuver the cursor to a target, then selects the target by \n",
      "pressing and releasing a button. simple as this seems, the interaction is even simpler \n",
      "with an eye tracker. the user locates the target by looking at it and follows \n",
      "immediately with selection [10].\n",
      "<p>\n",
      "\n",
      "although considerable research exists in eye tracking [3, 6, 9, 10], none has evaluated \n",
      "eye tracking with <i>iso 9241 ergonomic requirements for office work with visual \n",
      "display terminals (vdts) - part 9: requirements for non-keyboard input devices</i>. \n",
      "iso 9241-9 establishes uniform guidelines and testing procedures for evaluating \n",
      "computer pointing devices. the metric for comparison is <i>throughput</i>, in bits per \n",
      "second (bits/s), which includes both the speed and accuracy of users' performance. \n",
      "the equation for throughput is fitts' index of performance except using an <i>effective</i> \n",
      "index of difficulty (<i>id</i><sub>e</sub>). specifically,\n",
      "<p>\n",
      "\n",
      "<table width=90%>\n",
      "<tr>\n",
      "<td width=10%>\n",
      "<td width=80%>\n",
      "<i>throughput</i> = <i>id</i><sub>e</sub> / <i>mt</i>\n",
      "<td width=10% align=right>\n",
      "(1)\n",
      "</table>\n",
      "<p>\n",
      "\n",
      "where <i>mt</i> is the mean movement time, in seconds, for all trials within the same \n",
      "condition, and\n",
      "<p>\n",
      "\n",
      "<table width=90%>\n",
      "<tr>\n",
      "<td width=10%>\n",
      "<td width=80%>\n",
      "<i>id</i><sub>e</sub> = log<sub>2</sub>(<i>d</i> / <i>w</i><sub>e</sub> + 1).\n",
      "<td width=10% align=right>\n",
      "(2)\n",
      "</table>\n",
      "<p>\n",
      "\n",
      "<i>id</i><sub>e</sub>, in bits, is calculated from <i>d</i>, the distance to the target, and \n",
      "<i>w</i><sub>e</sub>, the effective width \n",
      "of the target. <i>w</i><sub>e</sub> is calculated as\n",
      "<p>\n",
      "\n",
      "<table width=90%>\n",
      "<tr>\n",
      "<td width=10%>\n",
      "<td width=80%>\n",
      "<i>w</i><sub>e</sub> = 4.133 × <i>sd</i>.\n",
      "<td width=10% align=right>\n",
      "(3)\n",
      "</table>\n",
      "<p>\n",
      "\n",
      "where <i>sd</i> is the standard deviation in the selection coordinates measured along the \n",
      "line from the center of the home square to the center of a target. using effective width \n",
      "allows throughput to incorporate the spatial variability in human performance. it \n",
      "includes both speed and accuracy [5].\n",
      "<p>\n",
      "\n",
      "<h3>prior evaluations</h3>\n",
      "\n",
      "iso 9241-9 was in draft international standard form in 1998 and became an \n",
      "international standard in 2000. if one considers mouse evaluations in research not \n",
      "following the standard, throughput ranged from about 2.6 bits/s to 12.5 bits/s. on the \n",
      "contrary, studies conforming to the standard reported throughput from about 3.7 bits/s \n",
      "to 4.9 bits/s [8]. the data appear much more uniform and consistent. in short, \n",
      "iso 9241-9 improves the quality and comparability of device evaluations. \n",
      "<p>\n",
      "\n",
      "although several papers follow iso 9241-9 and dozens of others use fitts' law to \n",
      "evaluate non-keyboard input devices, ware and mikaelian published in 1987 what \n",
      "remains the only fitts' law evaluation of an eye tracking system [10]. they used a \n",
      "serial fitts' law task to test three eye tracking techniques. task completion time was \n",
      "the only performance measure used. they compared eye tracking with the mouse but \n",
      "did not calculate or report on throughput as a performance measure. no eye tracking \n",
      "evaluation paper has ever been published since then using fitts' law (or iso 9241-9). \n",
      "<p>\n",
      "\n",
      "by following the standard and comparing throughput for eye tracking with a baseline \n",
      "technique (i.e., a mouse), we can determine how good an eye tracking system is. this \n",
      "paper is the first eye tracking evaluation conforming to iso 9241-9.\n",
      "<p>\n",
      "\n",
      "the rest of this paper is organized as follows. in section 2, we described the \n",
      "methodology of our experiment. in section 3, the results are presented and discussed. \n",
      "finally, we present our conclusions in section 4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Document5\n",
    "\n",
    "url = 'https://www.yorku.ca/mack/hcii2007.html'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('Document5.html', 'wb').write(r.content)\n",
    "d5 = open('Document5.html')\n",
    "d5 = d5.read()\n",
    "d5 = d5.lower()\n",
    "\n",
    "title5 = d5[(d5.index('<title>')+len('<title>')):(d5.index('\\n</title>'))]\n",
    "abstract5 = d5[(d5.index('<b>abstract</b><br>\\n')+len('<b>abstract</b><br>\\n')):(d5.index('\\n<p>\\n\\n<b>keywords</b>'))]\n",
    "introduction5 = d5[(d5.index('<h2>1 introduction</h2>\\n')+len('introduction</h2>\\n<p>\\n\\n')):(d5.index('\\n<p>\\n\\n<h2>2 methodology</h2>'))]\n",
    "\n",
    "\n",
    "print(title5+\"\\n\")\n",
    "print(abstract5+\"\\n\")\n",
    "print(introduction5+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 3, 5, 1, 1, 9]\n",
      "[1 1 0 1 3 5 1 1 9]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "words = ['auditory','and','visual']\n",
    "c1 = Counter(title.split())\n",
    "c2 = Counter(abstract.split())\n",
    "c3 = Counter(introduction.split())\n",
    "\n",
    "l = []\n",
    "for word in words:\n",
    "    l.append(c1[word])\n",
    "    l.append(c2[word])\n",
    "    l.append(c3[word])\n",
    "print(l)\n",
    "l = np.array(l)  \n",
    "l = l.transpose()\n",
    "print(l)\n",
    "\n",
    "l2 = []\n",
    "for i in range(0,len(words)):\n",
    "    l2.append(l[i::len(words)])\n",
    "l2 = np.array(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auditory</th>\n",
       "      <th>and</th>\n",
       "      <th>visual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abstract</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Introduction</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              auditory  and  visual\n",
       "Title                1    1       1\n",
       "Abstract             1    3       1\n",
       "Introduction         0    5       9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(l2, index = ['Title','Abstract','Introduction'], columns = words)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title.find('eye'))\n",
    "print(title2.find('eye'))\n",
    "print(title3.find('eye'))\n",
    "print(title4.find('eye'))\n",
    "print(title5.find('eye'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abstract.find('performance'))\n",
    "print(abstract2.find('performance'))\n",
    "print(abstract3.find('performance'))\n",
    "print(abstract4.find('performance'))\n",
    "print(abstract5.find('performance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "787\n",
      "-1\n",
      "648\n",
      "3843\n"
     ]
    }
   ],
   "source": [
    "print(introduction.find('method'))\n",
    "print(introduction2.find('method'))\n",
    "print(introduction3.find('method'))\n",
    "print(introduction4.find('method'))\n",
    "print(introduction5.find('method'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#Weighted zone\n",
    "titlew = 0.45\n",
    "absw = 0.30\n",
    "introw = 0.25\n",
    "q = ['eye','tracking']\n",
    "l = []\n",
    "for i in q:\n",
    "    if(title.find(i) != -1):\n",
    "        a = titlew\n",
    "    else:\n",
    "        a = 0\n",
    "    if(abstract.find(i) != -1):\n",
    "        b = absw\n",
    "        \n",
    "    else:\n",
    "        b = 0\n",
    "    if(introduction.find(i) != -1):\n",
    "        c = introw\n",
    "    else:\n",
    "        c = 0\n",
    "    d = a+b+c\n",
    "    l.append(d)\n",
    "print(sum(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25\n"
     ]
    }
   ],
   "source": [
    "titlew = 0.45\n",
    "absw = 0.30\n",
    "introw = 0.25\n",
    "q = ['eye','tracking']\n",
    "l = []\n",
    "for i in q:\n",
    "    if(title2.find(i) != -1):\n",
    "        a = titlew\n",
    "    else:\n",
    "        a = 0\n",
    "    if(abstract2.find(i) != -1):\n",
    "        b = absw\n",
    "        \n",
    "    else:\n",
    "        b = 0\n",
    "    if(introduction2.find(i) != -1):\n",
    "        c = introw\n",
    "    else:\n",
    "        c = 0\n",
    "    d = a+b+c\n",
    "    l.append(d)\n",
    "print(sum(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    }
   ],
   "source": [
    "titlew = 0.45\n",
    "absw = 0.30\n",
    "introw = 0.25\n",
    "q = ['eye','tracking']\n",
    "l = []\n",
    "for i in q:\n",
    "    if(title3.find(i) != -1):\n",
    "        a = titlew\n",
    "    else:\n",
    "        a = 0\n",
    "    if(abstract3.find(i) != -1):\n",
    "        b = absw\n",
    "        \n",
    "    else:\n",
    "        b = 0\n",
    "    if(introduction3.find(i) != -1):\n",
    "        c = introw\n",
    "    else:\n",
    "        c = 0\n",
    "    d = a+b+c\n",
    "    l.append(d)\n",
    "print(sum(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "titlew = 0.45\n",
    "absw = 0.30\n",
    "introw = 0.25\n",
    "q = ['eye','tracking']\n",
    "l = []\n",
    "for i in q:\n",
    "    if(title4.find(i) != -1):\n",
    "        a = titlew\n",
    "    else:\n",
    "        a = 0\n",
    "    if(abstract4.find(i) != -1):\n",
    "        b = absw\n",
    "        \n",
    "    else:\n",
    "        b = 0\n",
    "    if(introduction4.find(i) != -1):\n",
    "        c = introw\n",
    "    else:\n",
    "        c = 0\n",
    "    d = a+b+c\n",
    "    l.append(d)\n",
    "print(sum(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "titlew = 0.45\n",
    "absw = 0.30\n",
    "introw = 0.25\n",
    "q = ['eye','tracking']\n",
    "l = []\n",
    "for i in q:\n",
    "    if(title5.find(i) != -1):\n",
    "        a = titlew\n",
    "    else:\n",
    "        a = 0\n",
    "    if(abstract5.find(i) != -1):\n",
    "        b = absw\n",
    "        \n",
    "    else:\n",
    "        b = 0\n",
    "    if(introduction5.find(i) != -1):\n",
    "        c = introw\n",
    "    else:\n",
    "        c = 0\n",
    "    d = a+b+c\n",
    "    l.append(d)\n",
    "print(sum(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
