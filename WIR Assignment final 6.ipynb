{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    page=urlopen(url)\n",
    "    html_bytes = page.read()\n",
    "    html = html_bytes.decode(\"ISO-8859-1\")\n",
    "    return(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(html):\n",
    "    title_index = html.find(\"<title>\")\n",
    "    start_index = title_index + len(\"<title>\")\n",
    "    end_index = html.find(\"</title>\")\n",
    "    title = html[start_index+1:end_index-1]\n",
    "    return(title.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return (cleantext.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanlist(my_list):\n",
    "    removetable = str.maketrans('', '', '.,;:')\n",
    "    out_list = [s.translate(removetable) for s in my_list]\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOCUMENT 1\n",
    "\n",
    "html= get_page('https://www.yorku.ca/mack/chi03d.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1 Title :\n",
      " auditory and visual feedback during eye typing\n"
     ]
    }
   ],
   "source": [
    "d1_title=get_title(html)\n",
    "print('Document1 Title :\\n',d1_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1 Abstract :\n",
      " we describe a study on how auditory and visual feedback affects eye typing. results show that the feedback method influences both text entry speed and error rate. in addition, a proper feedback mode facilitates eye typing by reducing the user's need to switch her gaze between the on-screen keyboard and the typed text field.\n"
     ]
    }
   ],
   "source": [
    "start_index= html.find(\"We\")\n",
    "end_index=html.find('<p>')\n",
    "abstract=html[start_index:end_index-1]\n",
    "d1_abstract=abstract.lower()\n",
    "print('Document1 Abstract :\\n',d1_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1 Introduction :\n",
      " \n",
      "\n",
      "for people with severe disabilities their eyes may be the only means for communication. even though eye typing has been studied for many years, there is little research on design issues [2]. our goal was to study how feedback could facilitate the tedious [1] eye typing task and make gaze-based computer-aided communication more practical for those who need it.\n",
      "\n",
      "\n",
      "\n",
      "feedback modes\n",
      "\n",
      "\n",
      "\n",
      "during eye typing the user first focuses on the desired letter. to select the focused letter she continues to fixate on it thus using dwell time as an activation command. feedback is given for focus and selection. the following four feedback modes were tested. \n",
      "\n",
      "\n",
      "visual only. in the visual only mode, the key is highlighted on focus (the 2nd key on left in figure 1) and its symbol shrinks as dwell time elapses. the shrinking draws the attention in, helping the user focus on the center of the key. on selection the letter turns red and the key goes down. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "figure 1. animation for visual only feedback mode \n",
      "\n",
      "\n",
      "\n",
      "speech only. the speech only mode did not use visual feedback. the symbol on the key was spoken on selection.\n",
      "\n",
      "\n",
      "click plus visual. the click plus visual mode uses two modalities; it has the same visual feedback seen in figure 1 and, in addition, a short \"click\" is heard on selection.\n",
      "\n",
      "\n",
      "speech plus visual. the speech plus visual mode again uses the same visual feedback plus the symbol on the key is spoken on selection.\n",
      "\n",
      "the dwell time was the same for all modes: 400 ms before the start of the focus and 900 ms before selection. a summary of the modes follows.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "feedback mode\n",
      "while focused\n",
      "when selected\n",
      "\n",
      "\n",
      "visual only\n",
      "shrinking letter\n",
      "red letter, key down\n",
      "\n",
      "\n",
      "speech only\n",
      "none\n",
      "letter spoken\n",
      "\n",
      "\n",
      "click + visual\n",
      "shrinking letter\n",
      "red letter, key down, click\n",
      "\n",
      "\n",
      "speech + visual\n",
      "shrinking letter\n",
      "red letter, key down, \n",
      "letter spoken\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intro_index=html.find('INTRODUCTION')\n",
    "start_index = intro_index + len(\"INTRODUCTION\")\n",
    "\n",
    "end_index=html.find('METHOD')\n",
    "introduction=html[start_index+1:end_index]\n",
    "d1_introduction= cleanhtml(introduction)\n",
    "print('Document1 Introduction :\\n', d1_introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENT 2\n",
    "\n",
    "html= get_page('https://www.yorku.ca/mack/uais2006.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document2 Title :\n",
      " effects of feedback and dwell time on eye typing speed and accuracy\n"
     ]
    }
   ],
   "source": [
    "d2_title=get_title(html)\n",
    "print('Document2 Title :\\n',d2_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document2 Abstract :\n",
      " abstract: eye typing provides a means of communication\n",
      "that is especially useful for people with disabilities.\n",
      "however, most related research addresses technical issues\n",
      "in eye typing systems, and largely ignores design\n",
      "issues. this paper reports experiments studying the impact\n",
      "of auditory and visual feedback on user performance\n",
      "and experience. results show that feedback\n",
      "impacts typing speed, accuracy, gaze behavior, and\n",
      "subjective experience. also, the feedback should be\n",
      "matched with the dwell time. short dwell times require\n",
      "simplified feedback to support the typing rhythm,\n",
      "whereas long dwell times allow extra information on the\n",
      "eye typing process. both short and long dwell times\n",
      "benefit from combined visual and auditory feedback. six\n",
      "guidelines for designing feedback for gaze-based text\n",
      "entry are provided.\n"
     ]
    }
   ],
   "source": [
    "start_index= html.find(\"<b>\")\n",
    "end_index=html.find('provided.')\n",
    "abstract=html[start_index:end_index+len('provided.')]\n",
    "d2_abstract=cleanhtml(abstract)\n",
    "print('Document2 Abstract :\\n',d2_abstract)                       #ABSTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document2 Introduction :\n",
      " \n",
      "\n",
      "\n",
      "eye typing refers to the production of text using the\n",
      "focus of the eye (aka gaze) as a means of input. it is\n",
      "especially needed by people with severe disabilities,\n",
      "where controlling the eyes is sometimes the only means\n",
      "of interaction with the world. their need for an effective\n",
      "communication system is acute.\n",
      "\n",
      "\n",
      "research on technical aspects of eye typing extends\n",
      "over 20 years. however, there is little research on design\n",
      "issues [16]. the authorsÂ’ work [17, 18, 19] is an attempt\n",
      "to partly fill this gap. such work investigated how feedback\n",
      "can facilitate the tedious [5] eye typing task. this\n",
      "paper summarizes the results of three experiments\n",
      "studying various aspects of feedback during eye typing.\n",
      "because gaze has some unique features, the paper\n",
      "briefly introduces gaze as an input method. this is followed\n",
      "by a description of eye typing. examples of the\n",
      "most relevant research on eye gaze feedback are reviewed.\n",
      "the methods and results of the experiments are\n",
      "then presented, followed by guidelines gleaned from the\n",
      "results.\n"
     ]
    }
   ],
   "source": [
    "intro_index=html.find('Introduction')\n",
    "start_index = intro_index + len(\"Introduction\")\n",
    "\n",
    "end_index=html.find('results.')\n",
    "introduction=html[start_index:end_index+len('results.')]\n",
    "d2_introduction= cleanhtml(introduction)\n",
    "print('Document2 Introduction :\\n', d2_introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOCUMENT 3\n",
    "\n",
    "html=get_page('https://www.yorku.ca/mack/etra2014.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document3 Title :\n",
      " the use of gaze to control drones\n"
     ]
    }
   ],
   "source": [
    "d3_title=get_title(html)\n",
    "d3_title='the use of gaze to control drones'\n",
    "print('Document3 Title :\\n',d3_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document3 Abstract :\n",
      " this paper presents an experimental investigation of gaze-based control modes \n",
      "for unmanned aerial vehicles (uavs or Â“dronesÂ”). ten participants performed a \n",
      "simple flying task. we gathered empirical measures, including task completion \n",
      "time, and examined the user experience for difficulty, reliability, and fun. \n",
      "four control modes were tested, with each mode applying a combination of x-y \n",
      "gaze movement and manual (keyboard) input to control speed (pitch), altitude, \n",
      "rotation (yaw), and drafting (roll). participants had similar task completion \n",
      "times for all four control modes, but one combination was considered \n",
      "significantly more reliable than the others. we discuss design and performance \n",
      "issues for the gaze-plus-manual split of controls when drones are operated \n",
      "using gaze in conjunction with tablets, near-eye displays (glasses), or \n",
      "monitors.\n",
      "\n",
      "cr categories: information interfaces and presentation (e.g., hci): \n",
      "miscellaneous.\n",
      "\n",
      "keywords: drones, uav, gaze interaction, gaze input, multimodality, \n",
      "mobility, head-mounted displays, augmented or mixed reality systems, video \n",
      "gaming, robotics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_index= html.find(\"This\")\n",
    "end_index=html.find('</blockquote><p>')\n",
    "abstract=html[start_index:end_index+len('</blockquote><p>')]\n",
    "d3_abstract=cleanhtml(abstract)\n",
    "print('Document3 Abstract :\\n',d3_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document3 Introduction :\n",
      " \n",
      "\n",
      "\n",
      "unmanned aerial vehicles (uavs) or Â“dronesÂ” have a long history in military \n",
      "applications. they are able to carry heavy loads over long distances, while \n",
      "being controlled remotely by an operator. however, low-cost drones are now \n",
      "offering many non-military possibilities. these drones are light-weight, fly \n",
      "only a limited time (e.g., &lt; 20 minutes), and have limited range (e.g., &lt; 1 \n",
      "km). for instance, the off-the-shelf a.r. parrot drone costs around $400 and \n",
      "can be controlled from a pc, tablet, or smartphone (figure 1). it has a \n",
      "front-facing camera transmitting live-images to the pilot via wi-fi. people \n",
      "share videos, tips, and new software applications supported by its open-api.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "figure 1.\n",
      "commodity drone (a.r. parrot 2.0). the three\n",
      "main control axes are pitch, roll and yaw.\n",
      "\n",
      "\n",
      "this paper considers gaze as a potential input in the development of \n",
      "interfaces for drone piloting. when using gaze input for drone control, \n",
      "spatial awareness is directly conveyed without being mediated through another \n",
      "input device. the pilot sees what the drone sees. however, it is an open \n",
      "research question how best to include gaze in the command of drones. if gaze \n",
      "is too difficult to use, people may crash or lose the drones, which is \n",
      "dangerous and costly.\n",
      "\n",
      "\n",
      "\n",
      "first we present our motivation with examples of use-cases where gaze could \n",
      "offer a significant contribution. then we present previous research within the \n",
      "area. we designed an experiment to get feedback from users on their immediate, \n",
      "first-time impression of gaze-controlled flying. the experiment is presented \n",
      "in section 5. the paper then finishes with general discussion on how best to \n",
      "utilize gaze for drone control.\n"
     ]
    }
   ],
   "source": [
    "intro_index=html.find('Introduction')\n",
    "start_index = intro_index + len(\"Introduction\")\n",
    "\n",
    "end_index=html.find('control.')\n",
    "introduction=html[start_index:end_index+len('control.')]\n",
    "d3_introduction= cleanhtml(introduction)\n",
    "print('Document3 Introduction :\\n', d3_introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOCUMENT 4\n",
    "html=get_page('https://www.yorku.ca/mack/hcii2018b.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document4 Title :\n",
      " trackmaze: a comparison of head-tracking, eye-tracking, and tilt as input \n",
      "methods for mobile games\n"
     ]
    }
   ],
   "source": [
    "title_index = html.find(\"<title>\")\n",
    "start_index = title_index + len(\"<title>\")\n",
    "end_index = html.find(\"</title>\")\n",
    "title = html[start_index:end_index-1]\n",
    "d4_title= title.lower()\n",
    "print('Document4 Title :\\n',d4_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document4 Abstract :\n",
      " a user study was performed to compare three input methods (tilt, \n",
      "eye-tracking, head-tracking) with two gain levels (low, high) on a custom-made \n",
      "trackmaze mobile game. the task involved maneuvering a virtual ball through a maze \n",
      "while trying to avoid walls. the game was developed in swift using the arkit \n",
      "framework. the truedepth front-facing camera of an apple iphone x was used for the \n",
      "eye-tracking and head-tracking conditions. we evaluated user performance (maze \n",
      "completion time, number of wall hits) and qualitative measures (ease of use, \n",
      "enjoyment, fatigue). tilt input showed the best performance and eye-tracking \n",
      "showed the worst performance. the mean maze completion time was 12.3 seconds for \n",
      "tilt, 22.5 seconds for head-tracking, and 31.8 seconds for eye-tracking. high gain \n",
      "was 26% faster than low gain. tilt was the most precise input method with only \n",
      "1.06 wall hits per trial, compared to head-tracking (2.30) and eye-tracking \n",
      "(4.00). participants preferred tilt and head-tracking over eye-tracking and noted \n",
      "that the eye-tracking interface was fatiguing and hard to use.\n",
      "\n",
      "\n",
      "keywords: hci, mobile games, augmented reality on mobile devices, arkit, \n",
      "ios, head-tracking, eye-tracking, tilt-input\n"
     ]
    }
   ],
   "source": [
    "start_index= html.find(\"A user\")\n",
    "end_index=html.find('Tilt-input')\n",
    "abstract=html[start_index:end_index+len('Tilt-input')]\n",
    "d4_abstract=cleanhtml(abstract)\n",
    "print('Document4 Abstract :\\n',d4_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document4 Introduction :\n",
      " \n",
      "\n",
      "\n",
      "many computer users are switching from personal computers to smartphones or \n",
      "tablets for their daily computing needs. this transition has created growth in \n",
      "mobile application development, with many companies offering exclusive services \n",
      "and promotions over their mobile apps.  the goal is to lure users to smartphones. \n",
      "and the rewards are huge.  since launching the app store in 2008, apple's \n",
      "developer community has earned $70 billion worldwide, with $20 billion in 2016 \n",
      "alone [12, 13].  furthermore, gaming is the top-grossing across 25 app categories, \n",
      "with games accounting for 25% of all available apps [20].  this paper focuses on \n",
      "input methods for mobile games.\n",
      "\n",
      "\n",
      "  smartphones receive input through a touchscreen, a camera, microphone, \n",
      "  accelerometer, etc. touch input is the primary input method on smartphones, and \n",
      "  this is also true for mobile games. some drawbacks of touch input for the player \n",
      "  are lack of tactile feedback and occlusion of the display [8]. smartphones and \n",
      "  tablets typically include an accelerometer which measures the tilting motion and \n",
      "  the physical orientation of the device. device orientation can be used for \n",
      "  controlling game objects. such control, commonly called \"tilt\", is widely used \n",
      "  as an input method in mobile games. in addition, a smartphone's camera can track \n",
      "  movement in the real world. using the front-facing camera, we can track the \n",
      "  user's face or eye, and even obtain their facial expression. thus, eye-tracking \n",
      "  and head-tracking are possible input methods for hands-free games, in addition \n",
      "  to tilt.  one benefit of hands-free input is to provide more entertainment \n",
      "  options for people with special needs. however, using the eyes as an input \n",
      "  method is problematic, since the eyes are also our main perception method [16]. \n",
      "  notably, ease of use and novelty has an impact on \"perceived enjoyment\" in \n",
      "  mobile games [18]. \n",
      "\n",
      "\n",
      "  with ios 11, apple introduced arkit, a framework for creating augmented reality \n",
      "  (ar) apps for the iphone and ipad. arkit combined with the truedepth \n",
      "  front-facing camera available on the iphone x enables face or eye tracking in ar \n",
      "  apps. the truedepth camera gives the position, orientation, and expression of \n",
      "  the face in real-time. it also enables face id, the facial recognition method \n",
      "  for authentication on an iphone x. \n",
      "\n",
      "\n",
      "  in this paper, we evaluate the functionality of eye-tracking and head-tracking, \n",
      "  using arkit and the truedepth camera. we compare tilt-input, head-tracking, and \n",
      "  eye-tracking for a custom-designed game called trackmaze. with trackmaze, the \n",
      "  player navigates a virtual ball through a static maze. we address several \n",
      "  questions: is it possible to use eye-tracking and head-tracking for mobile games \n",
      "  that need whole-display navigation? can eye-tracking and head-tracking result in \n",
      "  better perceived enjoyment? can eye-tracking or head-tracking compete with \n",
      "  tilt-input, the natural input method for maze games? which input method is \n",
      "  easiest to use? do users prefer one input method over the others?\n",
      "\n",
      "\n",
      "  in the next section, we review studies on mobile games related to tilt-input, \n",
      "  eye-tracking, and head tracking. then, we discuss the methodology and results of \n",
      "  our user study. we then offer conclusions on user performance and enjoyment for \n",
      "  tilt, eye-tracking, and head-tracking as input methods.\n"
     ]
    }
   ],
   "source": [
    "intro_index=html.find('Introduction')\n",
    "start_index = intro_index + len(\"Introduction\")\n",
    "\n",
    "end_index=html.find('methods.')\n",
    "introduction=html[start_index:end_index+len('methods.')]\n",
    "d4_introduction= cleanhtml(introduction)\n",
    "print('Document4 Introduction :\\n', d4_introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOCUMENT 5\n",
    "\n",
    "html=get_page('https://www.yorku.ca/mack/hcii2007.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document5 Title :\n",
      " evaluating eye tracking with iso 9241 - part 9\n"
     ]
    }
   ],
   "source": [
    "d5_title=get_title(html)\n",
    "print('Document5 Title :\\n',d5_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document5 Abstract :\n",
      " the iso 9241-9 standard for computer pointing devices proposes an \n",
      "evaluation of performance and comfort [4]. this paper is the first eye tracking \n",
      "evaluation conforming to iso 9241-9. we evaluated three techniques and \n",
      "compared them with a standard mouse.\n"
     ]
    }
   ],
   "source": [
    "start_index= html.find(\"The\")\n",
    "end_index=html.find('mouse.')\n",
    "abstract=html[start_index:end_index+len('mouse.')]\n",
    "d5_abstract=cleanhtml(abstract)\n",
    "print('Document5 Abstract :\\n',d5_abstract)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document5 introduction :\n",
      " \n",
      "\n",
      "iso 9241 - part 9\n",
      "\n",
      "beginning with the apple macintosh in 1984, graphical user interfaces (guis) have \n",
      "evolved and matured. the key feature of modern guis is the ability for users to \n",
      "interact with simple point-and-select operations. the most common pointing device in \n",
      "desktop systems is the mouse. to select an on-screen target with a mouse, a user \n",
      "manipulates the mouse to maneuver the cursor to a target, then selects the target by \n",
      "pressing and releasing a button. simple as this seems, the interaction is even simpler \n",
      "with an eye tracker. the user locates the target by looking at it and follows \n",
      "immediately with selection [10].\n",
      "\n",
      "\n",
      "although considerable research exists in eye tracking [3, 6, 9, 10], none has evaluated \n",
      "eye tracking with iso 9241 ergonomic requirements for office work with visual \n",
      "display terminals (vdts) - part 9: requirements for non-keyboard input devices. \n",
      "iso 9241-9 establishes uniform guidelines and testing procedures for evaluating \n",
      "computer pointing devices. the metric for comparison is throughput, in bits per \n",
      "second (bits/s), which includes both the speed and accuracy of users' performance. \n",
      "the equation for throughput is fitts' index of performance except using an effective \n",
      "index of difficulty (ide). specifically,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "throughput = ide / mt\n",
      "\n",
      "(1)\n",
      "\n",
      "\n",
      "\n",
      "where mt is the mean movement time, in seconds, for all trials within the same \n",
      "condition, and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ide = log2(d / we + 1).\n",
      "\n",
      "(2)\n",
      "\n",
      "\n",
      "\n",
      "ide, in bits, is calculated from d, the distance to the target, and \n",
      "we, the effective width \n",
      "of the target. we is calculated as\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "we = 4.133 Ã— sd.\n",
      "\n",
      "(3)\n",
      "\n",
      "\n",
      "\n",
      "where sd is the standard deviation in the selection coordinates measured along the \n",
      "line from the center of the home square to the center of a target. using effective width \n",
      "allows throughput to incorporate the spatial variability in human performance. it \n",
      "includes both speed and accuracy [5].\n",
      "\n",
      "\n",
      "prior evaluations\n",
      "\n",
      "iso 9241-9 was in draft international standard form in 1998 and became an \n",
      "international standard in 2000. if one considers mouse evaluations in research not \n",
      "following the standard, throughput ranged from about 2.6 bits/s to 12.5 bits/s. on the \n",
      "contrary, studies conforming to the standard reported throughput from about 3.7 bits/s \n",
      "to 4.9 bits/s [8]. the data appear much more uniform and consistent. in short, \n",
      "iso 9241-9 improves the quality and comparability of device evaluations. \n",
      "\n",
      "\n",
      "although several papers follow iso 9241-9 and dozens of others use fitts' law to \n",
      "evaluate non-keyboard input devices, ware and mikaelian published in 1987 what \n",
      "remains the only fitts' law evaluation of an eye tracking system [10]. they used a \n",
      "serial fitts' law task to test three eye tracking techniques. task completion time was \n",
      "the only performance measure used. they compared eye tracking with the mouse but \n",
      "did not calculate or report on throughput as a performance measure. no eye tracking \n",
      "evaluation paper has ever been published since then using fitts' law (or iso 9241-9). \n",
      "\n",
      "\n",
      "by following the standard and comparing throughput for eye tracking with a baseline \n",
      "technique (i.e., a mouse), we can determine how good an eye tracking system is. this \n",
      "paper is the first eye tracking evaluation conforming to iso 9241-9.\n",
      "\n",
      "\n",
      "the rest of this paper is organized as follows. in section 2, we described the \n",
      "methodology of our experiment. in section 3, the results are presented and discussed. \n",
      "finally, we present our conclusions in section 4.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intro_index=html.find('Introduction')\n",
    "start_index = intro_index + len(\"Introduction\")\n",
    "\n",
    "end_index=html.find('Methodology')\n",
    "introduction=html[start_index:end_index-2]\n",
    "d5_introduction= cleanhtml(introduction)\n",
    "print('Document5 introduction :\\n', d5_introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_title_list=re.split('[- ]',d1_title)\n",
    "d1_title_list=cleanlist(d1_title_list)\n",
    "\n",
    "d1_abstract_list=re.split('[- ]',d1_abstract)\n",
    "d1_abstract_list=cleanlist(d1_abstract_list)\n",
    "\n",
    "d1_introduction_list=re.split('[- ]',d1_introduction)\n",
    "d1_introduction_list=cleanlist(d1_introduction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_title_list=re.split('[- ]',d2_title)\n",
    "d2_title_list=cleanlist(d2_title_list)\n",
    "\n",
    "d2_abstract_list=re.split('[- ]',d2_abstract)\n",
    "d2_abstract_list=cleanlist(d2_abstract_list)\n",
    "\n",
    "d2_introduction_list=re.split('[- ]',d2_introduction)\n",
    "d2_introduction_list=cleanlist(d2_introduction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3_title_list=re.split('[- ]', d3_title)\n",
    "d3_title_list=cleanlist(d3_title_list)\n",
    "\n",
    "d3_abstract_list=re.split('[- ]', d3_abstract)\n",
    "d3_abstract_list=cleanlist(d3_abstract_list)\n",
    "\n",
    "d3_introduction_list=re.split('[- ]', d3_introduction)\n",
    "d3_introduction_list=cleanlist(d3_introduction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4_title_list=re.split('[- ]', d4_title)\n",
    "d4_title_list=cleanlist(d4_title_list)\n",
    "\n",
    "d4_abstract_list=re.split('[- ]', d4_abstract)\n",
    "d4_abstract_list=cleanlist(d4_abstract_list)\n",
    "\n",
    "d4_introduction_list=re.split('[- ]', d4_introduction)\n",
    "d4_introduction_list=cleanlist(d4_introduction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "d5_title_list=re.split('[- ]', d5_title)\n",
    "d5_title_list=cleanlist(d5_title_list)\n",
    "\n",
    "d5_abstract_list=re.split('[- ]', d5_abstract)\n",
    "d5_abstract_list=cleanlist(d5_abstract_list)\n",
    "\n",
    "d5_introduction_list=re.split('[- ]', d5_introduction)\n",
    "d5_introduction_list=cleanlist(d5_introduction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zone Index for Document1: {'Abstract': (1, 3, 1), 'Introduction': (0, 6, 9), 'Title': (1, 1, 1)}\n"
     ]
    }
   ],
   "source": [
    "#ZONE INDEX\n",
    "\n",
    "dict1={}\n",
    "dict1['Abstract']=(d1_abstract_list.count('auditory'),d1_abstract_list.count('and'),d1_abstract_list.count('visual'))\n",
    "dict1['Introduction']=(d1_introduction_list.count('auditory'),d1_introduction_list.count('and'),d1_introduction_list.count('visual'))\n",
    "dict1['Title']=(d1_title_list.count('auditory'),d1_title_list.count('and'),d1_title_list.count('visual'))\n",
    "\n",
    "print('Zone Index for Document1:',dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document1: {'Abstract': 0, 'Introduction': 0, 'Title': 1}\n"
     ]
    }
   ],
   "source": [
    "#FINDING DOCUMENT\n",
    "\n",
    "dict2={}\n",
    "dict2['Abstract']=(d1_abstract_list.count('performance'))\n",
    "dict2['Introduction']=(d1_introduction_list.count('method'))\n",
    "dict2['Title']=(d1_title_list.count('eye'))\n",
    "print('Document1:',dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document2: {'Abstract': 0, 'Introduction': 1, 'Title': 1}\n"
     ]
    }
   ],
   "source": [
    "dict3={}\n",
    "dict3['Abstract']=(d2_abstract_list.count('performance'))\n",
    "dict3['Introduction']=(d2_introduction_list.count('method'))\n",
    "dict3['Title']=(d2_title_list.count('eye'))\n",
    "print('Document2:',dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document3: {'Abstract': 1, 'Introduction': 0, 'Title': 0}\n"
     ]
    }
   ],
   "source": [
    "dict4={}\n",
    "dict4['Abstract']=(d3_abstract_list.count('performance'))\n",
    "dict4['Introduction']=(d3_introduction_list.count('method'))\n",
    "dict4['Title']=(d3_title_list.count('eye'))\n",
    "print('Document3:',dict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document4: {'Abstract': 3, 'Introduction': 8, 'Title': 1}\n"
     ]
    }
   ],
   "source": [
    "dict5={}\n",
    "dict5['Abstract']=(d4_abstract_list.count('performance'))\n",
    "dict5['Introduction']=(d4_introduction_list.count('method'))\n",
    "dict5['Title']=(d4_title_list.count('eye'))\n",
    "print('Document4:',dict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document5: {'Abstract': 1, 'Introduction': 0, 'Title': 1}\n"
     ]
    }
   ],
   "source": [
    "dict6={}\n",
    "dict6['Abstract']=(d5_abstract_list.count('performance'))\n",
    "dict6['Introduction']=(d5_introduction_list.count('method'))\n",
    "dict6['Title']=(d5_title_list.count('eye'))\n",
    "print('Document5:',dict6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted zone score for Document1: 1.0\n"
     ]
    }
   ],
   "source": [
    "#WEIGHTED ZONE SCORE\n",
    "print('Weighted zone score for Document1:',(1*0.45+1*0.30+1*0.25)+(0*0.45+0*0.30+0*0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted zone score for Document2: 1.25\n"
     ]
    }
   ],
   "source": [
    "print('Weighted zone score for Document2:',(1*0.45+1*0.30+1*0.25)+(0*0.45+0*0.30+1*0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted zone score for Document3: 0.3\n"
     ]
    }
   ],
   "source": [
    "print('Weighted zone score for Document3:',(0*0.45+1*0.30+0*0.25)+(0*0.45+0*0.30+0*0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted zone score for Document4: 0.8\n"
     ]
    }
   ],
   "source": [
    "print('Weighted zone score for Document4:',(0*0.45+1*0.30+1*0.25)+(0*0.45+0*0.30+1*0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted zone score for Document5: 2.0\n"
     ]
    }
   ],
   "source": [
    "print('Weighted zone score for Document5:',(1*0.45+1*0.30+1*0.25)+(1*0.45+1*0.30+1*0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
